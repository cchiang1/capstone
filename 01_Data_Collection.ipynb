{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "The majority of my video game data was collected from Metacritic. I supplemented any missing and additional information from the RAWG API, such as missing genres, missing summary descriptions, and the URLs for the online stores where you can purchase each video games. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import regex as re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Names and Scores from Metacritic\n",
    "\n",
    "This scraping function gets the name, slug, metascore, user score, and release date for every video game on the page and returns it as a list of tuples. The parameters are your user agent and the link to the Metacritic page you want to scrape. (The slug is the part of the URL to the game's web page that includes the name of the game.)\n",
    "\n",
    "I used this function to get all the PC games that had a metascore from 2010 until 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for scraping the game names and basic info\n",
    "\n",
    "def get_games(user_agent, url):\n",
    "    game_list = []\n",
    "    \n",
    "    # Get request\n",
    "    # User-agent required or else will return a 403 status code\n",
    "    res = requests.get(url, headers={'User-Agent': user_agent})\n",
    "    soup = BeautifulSoup(res.content, 'lxml')\n",
    "    \n",
    "    # Get name, slug, scores, and date released for first game\n",
    "    # First game has a different class attribute than the rest \n",
    "    game = soup.find('li', {'class': 'product game_product first_product'})\n",
    "    \n",
    "    name = game.find('div', {'class': 'basic_stat product_title'}).text.strip()\n",
    "    slug = game.find('a')['href'].split('pc/', 1)[1]\n",
    "    meta = game.find('div', {'class': 'metascore_w small game positive'}).text\n",
    "    user = game.find('span', {'class': 'data'}).text\n",
    "    released = game.find_all('span', {'class': 'data'})[-1].text\n",
    "    \n",
    "    # Append it to list of games as a tuple\n",
    "    game_list.append((name, slug, meta, user, released))\n",
    "    \n",
    "    # Loop through the rest of the games on the page and append it to list\n",
    "    for i in range(len(soup.find_all('li', {'class': 'product game_product'}))):\n",
    "        game = soup.find_all('li', {'class': 'product game_product'})[i]\n",
    "        \n",
    "        name = game.find('div', {'class': 'basic_stat product_title'}).text.strip()\n",
    "        slug = game.find('a')['href'].split('pc/', 1)[1]\n",
    "        meta = game.find('div', {'class': 'metascore_w small game positive'}).text\n",
    "        user = game.find('span', {'class': 'data'}).text\n",
    "        released = game.find_all('span', {'class': 'data'})[-1].text\n",
    "        \n",
    "        game_list.append((name, slug, meta, user, released))\n",
    "    \n",
    "    # Return game info as a list of tuples\n",
    "    return game_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Genres\n",
    "\n",
    "To get the summary and genres for each game, I again used a function to access the Metacritic page for that game. The parameters for this function are the user agent and a list of slugs for every game.\n",
    "\n",
    "I scraped my data in batches in order to avoid 500 and 503 error codes, but if I did happen to get any sort of error code, the function would break out of the loop and return everything up until that error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for scraping the summary and genre of each game from Metacritic\n",
    "\n",
    "def get_summary_genre(user_agent, list_of_slugs):\n",
    "    game_details = []\n",
    "\n",
    "    # Loop through each game\n",
    "    for slug in list_of_slugs:\n",
    "        # Get request\n",
    "        url = f'https://www.metacritic.com/game/pc/{slug}'\n",
    "        res = requests.get(url, headers={'User-Agent': user_agent})\n",
    "        \n",
    "        # If page doesn't exist, return missing values for summary and genre \n",
    "        # and break out of loop\n",
    "        if res.status_code == 404:\n",
    "            print(f'Error. Status code: {res.status_code} for {slug}.')\n",
    "            summary = genres = np.nan\n",
    "            game_details.append((slug, summary, genres))\n",
    "            break\n",
    "            \n",
    "        # If 5xx error, break out of loop without saving that particular game\n",
    "        elif res.status_code >= 500:\n",
    "            print(f'Error. Status code: {res.status_code}. {slug} not scraped.')\n",
    "            break\n",
    "        \n",
    "        elif res.status_code == 200:\n",
    "            print(f'Status: {res.status_code}. Scraping {slug}...')\n",
    "            soup = BeautifulSoup(res.content, 'lxml')\n",
    "\n",
    "            # Summary\n",
    "            summ = soup.find('li', {'class': 'summary_detail product_summary'})\n",
    "\n",
    "            if summ == None:\n",
    "                summary = np.nan\n",
    "            else:\n",
    "                try:\n",
    "                    summary = summ.find('span', {'class': 'blurb blurb_expanded'}).get_text()\n",
    "                except:\n",
    "                    summary = summ.get_text()\n",
    "\n",
    "            # Genre\n",
    "            genre = soup.find('li', {'class': 'summary_detail product_genre'})\n",
    "            genre_list = genre.find_all('span', {'class': 'data'})\n",
    "            genres = set([genre_list[i].text for i in range(len(genre_list))])\n",
    "\n",
    "        game_details.append((slug, summary, genres))\n",
    "\n",
    "        # Lag time between requests\n",
    "        time.sleep(3)\n",
    "    \n",
    "    return game_details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ratings\n",
    "\n",
    "This function grabs the ESRB rating from the Metacritic page of each game. The parameters for this function are the user agent and list of slugs. Not all PC games have a rating because only console manufacturers are required to include a rating on their game. Ratings are optional for everyone else.\n",
    "\n",
    "Source: https://www.esrb.org/faqs/#are-all-games-required-to-have-a-rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for scraping the ESRB rating of each game from Metacritic\n",
    "\n",
    "def get_rating(user_agent, list_of_slugs):\n",
    "    rating_list = []\n",
    "    \n",
    "    for slug in list_of_slugs:\n",
    "        url = f'https://www.metacritic.com/game/pc/{slug}'\n",
    "        res = requests.get(url, headers={'User-Agent': user_agent})  \n",
    "        \n",
    "        # Return missing value if page doesn't exist\n",
    "        if res.status_code == 404:\n",
    "            print(f'{res.status_code} Error. No rating scraped.')\n",
    "            rating = np.nan\n",
    "        \n",
    "        # If 5xx status code, send another get request after a 3 sec lag\n",
    "        elif res.status_code >= 500:\n",
    "            print(f'{res.status_code} Error. Trying {slug} again.')\n",
    "            time.sleep(3)\n",
    "            \n",
    "            res = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(res.content, 'lxml')\n",
    "            \n",
    "            rate = soup.find('li', {'class': 'summary_detail product_rating'})\n",
    "            \n",
    "            # Return NaN if there is no rating\n",
    "            if rate == None:\n",
    "                rating = np.nan\n",
    "            else:\n",
    "                rating = rating.text.strip().replace('Rating:\\n', '')\n",
    "\n",
    "        elif res.status_code == 200:\n",
    "            print(f'Status: {res.status_code}. Scraping {slug}...')\n",
    "            \n",
    "            soup = BeautifulSoup(res.content, 'lxml')\n",
    "            rate = soup.find('li', {'class': 'summary_detail product_rating'})\n",
    "            \n",
    "            if rate == None:\n",
    "                rating = np.nan\n",
    "            else:\n",
    "                rating = rate.text.strip().replace('Rating:\\n', '')\n",
    "        \n",
    "        # Append rating to list\n",
    "        rating_list.append((slug, rating))\n",
    "        time.sleep(3)\n",
    "        \n",
    "    return rating_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAWG API\n",
    "\n",
    "I retrieved the data on every PC games in the RAWG database with these functions. These games were also collected in batches to avoid any errors.\n",
    "\n",
    "After that, I matched the API IDs to my list of Metacritic games in order to get the descriptions for each game from the API.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PC Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter results for PC games only \n",
    "def filter_pc(results):\n",
    "    pc = []\n",
    "    \n",
    "    for i in range(len(results)):\n",
    "        for platform in results[i]['platforms']:\n",
    "            if platform['platform']['id'] == 4:\n",
    "                pc.append(results[i])\n",
    "                break  \n",
    "    return pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for grabbing all the PC games from the RAWG API\n",
    "\n",
    "def get_api_games(key, start=1, end=500):\n",
    "    data = []\n",
    "    \n",
    "    # Loop through n pages (max page = 9942)\n",
    "    for i in range(start, end):\n",
    "        url = f'https://rawg-video-games-database.p.rapidapi.com/games?page={i}'\n",
    "\n",
    "        headers = {\n",
    "            'x-rapidapi-host': \"rawg-video-games-database.p.rapidapi.com\",\n",
    "            'x-rapidapi-key': key\n",
    "        }\n",
    "        \n",
    "        # Max results per page\n",
    "        params = {'page_size': 40}\n",
    "        \n",
    "        res = requests.request('GET', url, headers=headers, params=params)\n",
    "        games = res.json()['results']\n",
    "        \n",
    "        # Filter results for PC games only\n",
    "        pc = filter_pc(games)\n",
    "        data += pc\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting the game descriptions from RAWG API\n",
    "\n",
    "def get_description(key, api_id): \n",
    "    # Accounts for missing IDs\n",
    "    if api_id > 0:\n",
    "        url = f\"https://rawg-video-games-database.p.rapidapi.com/games/{int(api_id)}\"\n",
    "        headers = {\n",
    "            'x-rapidapi-host': \"rawg-video-games-database.p.rapidapi.com\",\n",
    "            'x-rapidapi-key': key\n",
    "        }\n",
    "        \n",
    "        res = requests.request('GET', url, headers=headers)\n",
    "        \n",
    "        # Gets rid of html tags\n",
    "        description = BeautifulSoup(res.json()['description'])\n",
    "        return description.get_text()\n",
    "    \n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stores and Images\n",
    "\n",
    "After cleaning and combining all the data I collected using the functions above into one dataframe, I went back and retrieved the URLs to the stores from RAWG and scraped the images from Metacritic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting the URLs to the online stores from RAWG API\n",
    "\n",
    "def get_store(key, api_id):\n",
    "    url = f\"https://rawg-video-games-database.p.rapidapi.com/games/{int(api_id)}\"\n",
    "    headers = {\n",
    "        'x-rapidapi-host': \"rawg-video-games-database.p.rapidapi.com\",\n",
    "        'x-rapidapi-key': key\n",
    "    }\n",
    "\n",
    "    res = requests.request('GET', url, headers=headers)\n",
    "    stores = res.json()['stores']\n",
    "    \n",
    "    # Grab link to Steam or Epic Games, two popular distrubtions services,\n",
    "    # if available. If not, grabs the first link in the list\n",
    "    if stores != []:\n",
    "        for i in range(len(stores)):\n",
    "            if 'Steam' in stores[i]['store']['name'] or 'Epic' in stores[i]['store']['name']:\n",
    "                return stores[i]['url']\n",
    "                break\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        return stores[0]['url']\n",
    "    \n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for scraping the image links from Metacritic\n",
    "def get_image(user_agent, slug):\n",
    "    url = f'https://www.metacritic.com/game/pc/{slug}'\n",
    "    res = requests.get(url, headers={'User-Agent': user_agent}) \n",
    "    \n",
    "    time.sleep(3)\n",
    "    \n",
    "    if res.status_code == 200:\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        image = soup.find('img', {'class': 'product_image large_image'})['src']\n",
    "\n",
    "        return image\n",
    "    else:\n",
    "        return np.nan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
